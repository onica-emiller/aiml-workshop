{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tank Filling Route Optimization with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Problem\n",
    "\n",
    "The travelling salesman problem (TSP) is a classic algorithmic problem in the field of computer science and operations research. Given a list of cities and the distances between each pair of cities, the problem is to find the most efficien t possible route that visits each city and returns to the origin city.\n",
    "\n",
    "In the classic version of the problem, the salesman picks a city to start, travels through remaining cities and returns to the original city. \n",
    "\n",
    "In this version, we have slightly modified the problem, presenting it as a tank filling delivery problem on a 2D gridworld.  The agent (driver) starts at the chemical depot, a fixed point on the grid.  Then, the tanks appear elsewhere on the grid.  The driver needs to visit the the tanks to obtain rewards.  Rewards are proportional to the time taken to do this (equivalent to the distance, as each timestep moves one square on the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Version\n",
    "In the Easy Version, we are on a very small grid.  All tank locations are generated at the start of the episode.  Tank locations are fixed, and are invariant (non-random) from episode to episode.  The objective is to visit each tank location, and return to the depot.  We have a maximum time-limit of 50 steps.  \n",
    "\n",
    "### States\n",
    "At each time step, our agent is aware of the following information:\n",
    "\n",
    "1. For the Chemical Depot:\n",
    "    1. Location (x,y coordinates)\n",
    "    \n",
    "2. For the Driver\n",
    "    1. Location (x,y coordinates)\n",
    "    2. Is driver at depot (yes/no)\n",
    "\n",
    "3. For each Tank: \n",
    "    1. Location (x,y coordinates)\n",
    "    2. Status (Delivered or Not Delivered)\n",
    "    3. Time (Time taken to deliver reach tank -- incrementing until delivered)\n",
    "4. Miscellaneous\n",
    "    1. Time since start of episode\n",
    "    2. Time remaining until end of episode (i.e. until max time)\n",
    "\n",
    "### Actions\n",
    "At each time step, our agent can take the following steps:\n",
    "- Up - Move one step up in the map\n",
    "- Down - Move one step down in the map\n",
    "- Right - Move one step right in the map\n",
    "- Left - Move one step left in the map\n",
    "\n",
    "### Rewards\n",
    "Agent gets a reward of -1 for each time step. If an tank is visited in that timestep, it gets a positive reward inversely proportional to the time taken to deliver. If all the tanks are visited and the agent is back to the depot, it gets an additional reward inversely proportional to time since start of episode. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using AWS SageMaker for RL\n",
    "\n",
    "AWS SageMaker allows you to train your RL agents in cloud machines using docker containers. You do not have to worry about setting up your machines with the RL toolkits and deep learning frameworks. You can easily switch between many different machines setup for you, including powerful GPU machines that give a big speedup. You can also choose to use multiple machines in a cluster to further speedup training, often necessary for production level loads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites \n",
    "\n",
    "#### Imports\n",
    "\n",
    "To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_type = 'tsp-easy'\n",
    "\n",
    "# create unique job name \n",
    "job_name_prefix = 'rl-' + env_type\n",
    "\n",
    "# S3 bucket\n",
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an IAM role\n",
    "Either get the execution role when running from a SageMaker notebook `role = sagemaker.get_execution_role()` or, when running from local machine, use utils method `role = get_execution_role('role_name')` to create an execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the environment\n",
    "\n",
    "The environment is defined in a Python file called “TSP_env.py” and the file is uploaded on /src directory. \n",
    "\n",
    "The environment also implements the init(), step(), reset() and render() functions that describe how the environment behaves. This is consistent with Open AI Gym interfaces for defining an environment. \n",
    "\n",
    "\n",
    "1. Init() - initialize the environment in a pre-defined state\n",
    "2. Step() - take an action on the environment\n",
    "3. reset()- restart the environment on a new episode\n",
    "4. render() - get a rendered image of the environment in its current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs. \n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code \n",
    "3. Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET or the RLRAY_PRESET can be used to specify the RL agent algorithm you want to use. \n",
    "6. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "  \n",
    "estimator = RLEstimator(entry_point=\"train-coach.py\",\n",
    "                        source_dir='src',\n",
    "                        dependencies = [\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.COACH,\n",
    "                        toolkit_version='0.11.0',\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        role=role,\n",
    "                        train_instance_type=\"ml.m4.4xlarge\",\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        hyperparameters = {\n",
    "                          #expected run time 12 mins for TSP Easy  \n",
    "                          \"RLCOACH_PRESET\": \"preset-\" + env_type, \n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store intermediate training output and model checkpoints \n",
    "\n",
    "The output from the training job above is stored on S3. The intermediate folder contains gifs and metadata of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name=estimator._current_job_name\n",
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate\".format(job_name)\n",
    "output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Output.tar.gz location: {}\".format(output_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing against a baseline policy\n",
    "https://github.com/NervanaSystems/coach#benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline reward\n",
    "#os.chdir(\"src\")\n",
    "from TSP_env import TSPEasyEnv\n",
    "from TSP_baseline import get_mean_baseline_reward\n",
    "baseline_mean, baseline_std_dev = get_mean_baseline_reward(env=TSPEasyEnv(),num_of_episodes=1)\n",
    "print(baseline_mean,baseline_std_dev)\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot metrics for training job\n",
    "We can pull the reward metric of the training and plot it to see the performance of the model over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# csv_file has all the RL training metrics\n",
    "csv_file = \"{}/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\".format(tmp_dir)\n",
    "csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\"\n",
    "key = intermediate_folder_key + \"/\" + csv_file_name\n",
    "wait_for_s3_object(s3_bucket, key, tmp_dir)\n",
    "\n",
    "csv_file = \"{}/{}\".format(tmp_dir, csv_file_name)\n",
    "df = pd.read_csv(csv_file)\n",
    "x_axis = 'Episode #'\n",
    "y_axis_rl = 'Training Reward'\n",
    "y_axis_base = 'Baseline Reward'\n",
    "df[y_axis_rl] = df[y_axis_rl].rolling(5).mean()\n",
    "df[y_axis_base] = baseline_mean\n",
    "y_axes = [y_axis_rl]\n",
    "ax = df.plot(x=x_axis,y=[y_axis_rl,y_axis_base], figsize=(18,6), fontsize=18, legend=True, color=['b','r'])\n",
    "fig = ax.get_figure()\n",
    "ax.set_xlabel(x_axis,fontsize=20)\n",
    "#ax.set_ylabel(y_axis,fontsize=20)\n",
    "#fig.savefig('training_reward_vs_wall_clock_time.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the rendered gifs\n",
    "The latest gif file found in the gifs directory is displayed. You can replace the tmp.gif file below to visualize other files generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = intermediate_folder_key + '/gifs'\n",
    "wait_for_s3_object(s3_bucket, key, tmp_dir)    \n",
    "print(\"Copied gifs files to {}\".format(tmp_dir))\n",
    "\n",
    "glob_pattern = os.path.join(\"{}/*.gif\".format(tmp_dir))\n",
    "gifs = [file for file in glob.iglob(glob_pattern, recursive=True)]\n",
    "extract_episode = lambda string: int(re.search('.*episode-(\\d*)_.*', string, re.IGNORECASE).group(1))\n",
    "gifs.sort(key=extract_episode)\n",
    "print(\"GIFs found:\\n{}\".format(\"\\n\".join([os.path.basename(gif) for gif in gifs])))    \n",
    "\n",
    "# visualize a specific episode\n",
    "#gif_index = -1 # since we want last gif\n",
    "#gif_filepath = gifs[gif_index]\n",
    "#gif_filename = os.path.basename(gif_filepath)\n",
    "#print(\"Selected GIF: {}\".format(gif_filename))\n",
    "#os.system(\"mkdir -p ./src/tmp_render/ && cp {} ./src/tmp_render/{}.gif\".format(gif_filepath, gif_filename))\n",
    "#HTML('<img src=\"./src/tmp_render/{}.gif\">'.format(gif_filename))\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "for gif in gifs:\n",
    "    print(gif.split('_')[2].split('.')[0])\n",
    "    display(Image(filename=gif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
