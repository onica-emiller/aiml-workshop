{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morse Code Lab\n",
    "This lab uses data produced by using the IoTanium device as a Morse code signal generator.  This notebook demonstrates the use of an SKLearn model, which will be trained to recognize particular signals. \n",
    "\n",
    "In this lab, the data have already been generated by manually tapping out Morse code patterns on the IoTanium button.  Each of two patterns were tapped out many times and recorded.  These recordings are stored as a sequence of `(time, value)` pairs, using the IoTanium onboard clock and the button value (1=pressed, 0=not pressed).  \n",
    "\n",
    "The two sequences recorded are:\n",
    "- `....  ..` = `HI`\n",
    "- `...  ---  ...` = `SOS` (international nautical/aviation distress signal)\n",
    "\n",
    "You can learn more about Morse code here: https://en.wikipedia.org/wiki/Morse_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name='iotanium-test'\n",
    "prefix='iotanium/data_7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role and session information needed for the .fit and .deploy operations by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump out a list of all the available data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list=s3_client.list_objects(Bucket=bucket_name, Prefix=prefix)\n",
    "files=[]\n",
    "for contents in obj_list['Contents']:\n",
    "    files.append(contents['Key'])\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to read the JSON data files and extract the (time, value) tuples to arrays `x` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as fh:\n",
    "        raw = json.load(fh)\n",
    "        data = raw['records']\n",
    "        rawx,rawy = zip(*data)\n",
    "        \n",
    "        #-- subtract off initial timestamp so x starts at zero\n",
    "        x = np.array(rawx)-rawx[0]\n",
    "        y = np.array(rawy)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These function does the necessary trimming of the recorded signal to +/- 250 millisec of the leading edge and trailing edge.  The result is then interpolated to a uniformly spaced array of 500 values.\n",
    "\n",
    "There is one function for the using the touch sensor (where touch drops values) and one for button sensor (where push increases values, from 0 to 1).  The one to invoke is wrapped in `prepare_data(x,y)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_touch(x,y):\n",
    "\n",
    "    #-- define a threshold to determine leading/trailing exceedance edge\n",
    "    my = np.median(y)\n",
    "    thresh = 0.75 * my\n",
    "    \n",
    "    #-- find leading/trailing edges\n",
    "    x0 = np.min(x[y<thresh]) if np.any(y<thresh) else np.min(x)\n",
    "    xN = np.max(x[y<thresh]) if np.any(y<thresh) else np.max(x)\n",
    "    \n",
    "    #-- start and end a bit before/after edges\n",
    "    x0 -= 250\n",
    "    xN += 250\n",
    "    \n",
    "    xn = (x-x0) / float(xN-x0)\n",
    "    yn = y[(xn>=0) & (xn<=1)]\n",
    "    xn = xn[(xn>=0) & (xn<=1)]\n",
    "    \n",
    "    newx = np.arange(500)*0.002\n",
    "    newy = np.interp(newx,xn,yn)\n",
    "    return newx,newy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_button(x,y):\n",
    "\n",
    "    #-- define a threshold to determine leading/trailing exceedance edge\n",
    "    thresh = 0.5\n",
    "    \n",
    "    #-- find leading/trailing edges\n",
    "    x0 = np.min(x[y>thresh]) if np.any(y>thresh) else np.min(x)\n",
    "    xN = np.max(x[y>thresh]) if np.any(y>thresh) else np.max(x)\n",
    "    \n",
    "    #-- start and end a bit before/after edges\n",
    "    x0 -= 250\n",
    "    xN += 250\n",
    "    \n",
    "    xn = (x-x0) / float(xN-x0)\n",
    "    yn = y[(xn>=0) & (xn<=1)]\n",
    "    xn = xn[(xn>=0) & (xn<=1)]\n",
    "    \n",
    "    newx = np.arange(500)*0.002\n",
    "    newy = np.interp(newx,xn,yn)\n",
    "    return newx,newy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which sensor will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x,y):\n",
    "    return prepare_data_button(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(prefix, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data files to the local notebook instance, perform data preparation and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in files:\n",
    "    s3_loc = 's3://{}/{}'.format(bucket_name, filename)\n",
    "    s3.Bucket(bucket_name).download_file(filename, filename)\n",
    "    rawx,rawy = read_data(filename)\n",
    "    x,y = prepare_data(rawx,rawy)\n",
    "    plt.plot(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the wrapper that lets SageMaker treat SKLearn algorithms as if they were SageMaker \"native\" models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = 'morse_kmeans2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize morse_kmeans2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the object which will act as the model.  This will wrap the external Python code (which contains the same cleaning functions above) and the SKLearn's version of KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.m4.xlarge'\n",
    "#instance_type = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=instance_type,\n",
    "    role=role,\n",
    "    hyperparameters={'n_clusters': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the SKLearn model to the data (i.e. determine clusters).  This will spin up and spin down the instance type specified above, and can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({'train': 's3://iotanium-test/iotanium/data_7'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the model artifacts location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Model\n",
    "SageMaker always stores the model artifacts are stored in S3, regardless of whether the model was trained locally or on cloud instances. In order to unpack the details of the model, we have to make a copy of the model artifacts on the notebook and load the object. Fortunately, SKLearn models are open source, so we examine the model here. Note this sub section is purely diagnostic curiousity, and not strictly part of a train/deploy operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = urllib.parse.urlsplit(sklearn.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(u.netloc, u.path[1:], 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skmodel = joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what the \"cluster centers\" look like; these are basically what the model sees as the nominal version of each signal and what any new data will be compared against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(skmodel.cluster_centers_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(skmodel.cluster_centers_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine how model labeled the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skmodel.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the endpoint. This will take in the JSON sent from the IoT Core->Lambda function->endpoint path and return a JSON response (received and modified by the Lambda function). Note that this instance must be deleted manually when you're done messing with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.Session().client(service_name='runtime.sagemaker',region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the endpoint.  This must be added to the Lambda function environment so it knows where to send data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor._get_endpoint_config_name()\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get one of the available files (used for training, but just to see what happens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0], 'rb') as f:\n",
    "    payload = f.read()\n",
    "#payload = bytearray(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the file to the endpoint to examine the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_body.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Delete the endpoint when you're done to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
